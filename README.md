# ğŸ’³ Credit Card Transactional Analysis for Fraud Risk

A robust data engineering pipeline designed to detect potential fraud in credit card transactions using **PySpark on GCP Dataproc Serverless**. The system processes millions of daily records, enriches them via BigQuery, and orchestrates the entire flow with Airflow (GCP Composer). CI/CD is fully automated with GitHub Actions and PyTest to ensure code quality and deployment reliability.

> âœ… Automated fraud risk evaluation with PySpark  
> âœ… Improved test & deployment trust by **35%** through GitHub Actions CI/CD  
> âœ… Serverless architecture with **GCP-native services** for seamless scalability

---

## ğŸ› ï¸ Tech Stack

- **Languages**: Python
- **Processing**: PySpark (GCP Dataproc Serverless)
- **Data Sources**: Google Cloud Storage (GCS), BigQuery
- **Orchestration**: Apache Airflow via GCP Composer
- **Testing**: PyTest
- **CI/CD**: GitHub Actions

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ credit_fraud_dag.py             # Airflow DAG definition
â”œâ”€â”€ pyspark_jobs/
â”‚   â””â”€â”€ fraud_detection_job.py         # PySpark script for fraud detection
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_fraud_logic.py            # PyTest unit tests
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml                  # GitHub Actions CI/CD pipeline
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_transactions.json       # Sample input data (GCS)
â”œâ”€â”€ README.md                          # Project documentation
â””â”€â”€ requirements.txt                   # Python dependencies
```

---

## âš™ï¸ Pipeline Workflow

1. **BigQuery Setup**
   - Create a static table containing cardholder information.

2. **PySpark Processing**
   - Reads:
     - Cardholder info from BigQuery
     - Daily transactional data (JSON) from GCS
   - Applies:
     - Data validation rules
     - Transformation logic for fraud risk scoring
   - Outputs:
     - Processed data written back to GCS (archive or results)

3. **Airflow DAG (GCP Composer)**
   - Detects new transactional data in GCS
   - Triggers the PySpark job on **Dataproc Serverless**
   - Archives the processed files after completion

4. **CI/CD Automation**
   - GitHub Actions pipeline:
     - Runs PyTest unit tests on push
     - Deploys DAGs and PySpark scripts to GCS bucket used by Composer

---

## ğŸ§ª Sample PyTest

```python
def test_high_value_transaction_flag():
    transaction = {"amount": 12000, "location": "NY", "user_id": "123"}
    risk = calculate_fraud_score(transaction)
    assert risk["risk_score"] > 0.8
```

---

## ğŸ”„ GitHub Actions CI/CD Workflow

- Triggered on every push or PR to `main` branch
- Executes:
  - `pytest` on `/tests`
  - Uploads:
    - DAG files to `gs://composer-dags-bucket/dags/`
    - PySpark jobs to `gs://dataproc-code-bucket/jobs/`

---

## ğŸ“œ License

This project is licensed under the MIT License.

---

## ğŸ¤ Contributing

Open to contributions! Please fork the repo and raise a pull request.

---

## ğŸ“¬ Contact

For any questions or suggestions, connect via [LinkedIn](https://www.linkedin.com/in/h-v-m-) or raise an issue.
